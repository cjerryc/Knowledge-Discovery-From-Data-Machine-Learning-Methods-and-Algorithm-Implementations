{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-KegHs4e7IPtd7SQgF1U2zwVwY0_uH2W","timestamp":1710199093211}],"mount_file_id":"1Cq9wIkuEyfaSe6sLXUL7XsaFaU6fRAKJ","authorship_tag":"ABX9TyMdYmwUFHCzl3zMuGEAPj+R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6ap2R13d_oU","executionInfo":{"status":"ok","timestamp":1710224408167,"user_tz":420,"elapsed":262,"user":{"displayName":"Jerry Chang","userId":"16741169191998840800"}},"outputId":"e11f5381-15c4-40de-8ef8-9e7799250fca"},"outputs":[{"output_type":"stream","name":"stdout","text":["['A', 'A', 'A']\n","['B', 'B', 'A']\n","The Optimal Policy is: ['B', 'B', 'A'] for states [S1, S2, S3], respectively.\n"]}],"source":["# This program uses Policy Iteration to find the the optimal policy.\n","# in a Markov Decision Process. Value Iteration on MDP is a GREEDY Algorithm (It will take the highest reward at each step).\n","# Note: This program is specific to the Markov Decision Process as described below. For more states or actions, the code must be adjusted to\n","# incorporate Action A, Action B, ..., Action N. For different states,\n","import pandas as pd\n","\n","# transition_probabilities_A = transition probabilities for Action A, transition_probabilities_B = transition probabilities for Action B\n","# transition_probabilities_A = [s1[s1, s2, s3], s2[s1, s2, s3], s3[s1, s2, s3]]\n","# transition_probabilities_B = [s1[s1, s2, s3], s2[s1, s2, s3], s3[s1, s2, s3]]\n","# [s1, s2, s3] original state rewards\n","\n","\n","def get_transition_probabilities_and_rewards_from_csv(transitionFile_A, transitionFile_B, rewardFile):\n","  # Read in CSV files\n","  transition_A_df = pd.read_csv(transitionFile_A, header=None)\n","  transition_B_df = pd.read_csv(transitionFile_B, header=None)\n","  rewards_df = pd.read_csv(rewardFile, header=None)\n","\n","  # Process Markov State Transition Probabilities, ACTION A\n","  transition_probabilities_A = []\n","  for i in range(len(transition_A_df)):\n","    temp = []\n","    ser = pd.Series(transition_A_df.iloc[i])\n","    for j in range(transition_A_df.shape[0]):\n","      temp.append(ser.iloc[j])\n","    transition_probabilities_A.append(temp)\n","\n","  # Process Markov State Transition Probabilities, ACTION B\n","  transition_probabilities_B = []\n","  for i in range(len(transition_B_df)):\n","    temp = []\n","    ser = pd.Series(transition_B_df.iloc[i])\n","    for j in range(transition_B_df.shape[0]):\n","      temp.append(ser.iloc[j])\n","    transition_probabilities_B.append(temp)\n","\n","  # Process Markov State Rewards\n","  state_reward = []\n","  for i in range(len(rewards_df)):\n","    ser = pd.Series(rewards_df.iloc[i])\n","    state_reward.append(ser.iloc[0])\n","\n","  return transition_probabilities_A, transition_probabilities_B, state_reward\n","\n","\n","# Given a particular state S_i, calculate the J*(S_i) Reward\n","def J_star_reward(state, original_state_rewards, prior_state_rewards, gamma, transition_prob_A, transition_prob_B, policyAction='A'):\n","  reward = original_state_rewards[state]\n","  Jsum = 0\n","  for i in range(len(prior_state_rewards)):\n","    if policyAction == 'A':\n","      Jsum += transition_prob_A[state][i] * prior_state_rewards[i]  # Calculate a Jreward for the action.\n","    else:\n","      Jsum += transition_prob_B[state][i] * prior_state_rewards[i]\n","\n","  reward +=  gamma * Jsum\n","\n","  return reward\n","\n","\n","# Given a particular state S_i, calculate the J*(S_i) Reward\n","def policy_improvement_J_star_reward(state, original_state_rewards, prior_state_rewards, gamma, transition_prob_A, transition_prob_B):\n","  reward = original_state_rewards[state]\n","  Jsum_A = 0\n","  Jsum_B = 0\n","  for i in range(len(prior_state_rewards)):\n","      Jsum_A += transition_prob_A[state][i] * prior_state_rewards[i]  # Calculate a Jreward for each action, and take the MAX for most reward gain.\n","      Jsum_B += transition_prob_B[state][i] * prior_state_rewards[i]\n","\n","  reward_A = reward + gamma * Jsum_A\n","  reward_B = reward + gamma * Jsum_B\n","\n","  if reward_A > reward_B:\n","    return 'A'\n","  else:\n","    return 'B'\n","\n","\n","# Given an Epsilon threshold, calculate the Markov System Value Iteration Reward until the difference between consecutive\n","# rewards is smaller than Epsilon.\n","def MDP_Policy_Iteration_Helper(Epsilon, discountFactor, transitionProbabilities_A, transition_probabilities_B, stateRewards, est_policy):\n","  # At K=1 iteration, the state reward is the reward\n","  discounted_rewards = stateRewards\n","\n","  while True:\n","      # Iterate through every State and calculate the discounted rewards against epsilon\n","      temp = []  # temp holds the value of the current iteration-K rewards.\n","      for i in range(len(discounted_rewards)):\n","          temp.append(J_star_reward(i, stateRewards, discounted_rewards, discountFactor, transitionProbabilities_A, transition_probabilities_B, est_policy[i]))\n","\n","      # Find the Max difference between K and K-1 iterations of Rewards, and compare it to Epsilon.\n","      # If the difference is smaller than Epsilon, stop calculating. The discounted rewards have converged.\n","      Max_diff = 0\n","      for i in range(len(temp)):\n","          curr_diff = abs(temp[i] - discounted_rewards[i])\n","          Max_diff = curr_diff if curr_diff > Max_diff else Max_diff\n","      # Check to see if Error threshold has been reached for max difference between subsequent k-iterations. If so, the rewards have converged.\n","      # Return Long Term Discounted Rewards and the Policy.\n","      # Round the calculated Discounted Rewards to 2 decimal places.\n","      if Max_diff < Epsilon:\n","          longTemDiscountedRewards = [round(elem, 2) for elem in temp]\n","          return longTemDiscountedRewards\n","\n","      discounted_rewards = temp\n","\n","\n","def Markov_Decision_Process_Policy_Iteration(Epsilon, discountFactor, transProb_A, transProb_B, stateRewards, initPolicy):\n","  initialPolicy = initPolicy   # Policy to feed into the Policy Iteration task.\n","\n","  while True:\n","    # (1) Policy Evaluation: Calculate the Long Term Discounted Rewards of the Markov Decision Problem\n","    long_term_discounted_rewards = MDP_Policy_Iteration_Helper(Epsilon, discountFactor, transProb_A, transProb_B, stateRewards, initialPolicy)\n","    # (2) Policy Improvement\n","    updated_policy = []\n","    for i in range(len(long_term_discounted_rewards)):\n","      updated_policy.append(policy_improvement_J_star_reward(i, stateRewards, long_term_discounted_rewards, discountFactor, transProb_A, transProb_B))\n","\n","    print(initialPolicy)\n","    # Check to see if policy is changed. If changed, continue to iterate. It unchanged, the Optimal Policy has been found; return findings.\n","    if initialPolicy == updated_policy:\n","      return updated_policy\n","    else:\n","      initialPolicy = updated_policy\n","\n","\n","\n","\n","# Read in the MDP information, consisting of transition probabilities per ACTION (A, B in this exampe MDP)and state rewards\n","epsilon = 0.001\n","discount_factor = 0.9\n","initial_policy = ['A', 'A', 'A']\n","markovTransitionFileName_A = '/content/drive/MyDrive/MDP_A Transition Probabilities - Sheet1.csv'\n","markovTransitionFileName_B = '/content/drive/MyDrive/MDP_B Transition Probabilities - Sheet1.csv'\n","markovRewardFileName = '/content/drive/MyDrive/MDP State Rewards - Sheet1.csv'\n","trans_prob_A, trans_prob_B, st_rewards = get_transition_probabilities_and_rewards_from_csv(markovTransitionFileName_A, markovTransitionFileName_B, markovRewardFileName)\n","\n","# Run Policy Iteration\n","optimal_policy = Markov_Decision_Process_Policy_Iteration(epsilon, discount_factor, trans_prob_A, trans_prob_B, st_rewards, initial_policy)\n","print(f\"The Optimal Policy is: {optimal_policy} for states [S1, S2, S3], respectively.\")\n","\n","\n"]}]}