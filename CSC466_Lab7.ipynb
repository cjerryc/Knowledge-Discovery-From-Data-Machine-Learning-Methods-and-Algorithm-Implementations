{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qobxbuiQD_Nd5StP3yGNTzTj1w7qRQSh","authorship_tag":"ABX9TyOnMq0msES3uXzeRpymua0+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YAuioTUum_2l","executionInfo":{"status":"ok","timestamp":1710193606847,"user_tz":420,"elapsed":894,"user":{"displayName":"Jerry Chang","userId":"16741169191998840800"}},"outputId":"2c925e9c-6728-44db-8705-cfc610ea4fa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Discounted Rewards are: [4.8, -1.6, -11.2]\n"]}],"source":["# This program uses Value Iteration to find the long-term Discounted Rewards of each state of a Markov System.\n","import pandas as pd\n","# [sun, wind, hail] transition probabilities matrix, 2D\n","# Ex. sun -> sun probability is at transition_probabilities[0][0]\n","# Ex. wind -> hail probability is at transition_probabilities[1][2]\n","# transition_probabilities = [[0.5, 0.5, 0], [0.5, 0, 0.5], [0, 0.5, 0.5]]\n","# [sun, wind, hail] original state rewards\n","\n","\n","\n","def get_transition_probabilities_and_rewards_from_csv(transitionFile, rewardFile):\n","  # Read in CSV files\n","  transition_df = pd.read_csv(transitionFile)\n","  rewards_df = pd.read_csv(rewardFile)\n","\n","  # Process Markov State Transition Probabilities\n","  transition_probabilities = []\n","  for i in range(len(transition_df)):\n","    temp = []\n","    ser = pd.Series(transition_df.iloc[i])\n","    for j in range(transition_df.shape[0]):\n","      temp.append(ser.iloc[j])\n","    transition_probabilities.append(temp)\n","\n","  # Process Markov State Rewards\n","  state_reward = []\n","  for i in range(len(rewards_df)):\n","    ser = pd.Series(rewards_df.iloc[i])\n","    state_reward.append(ser.iloc[0])\n","\n","  return transition_probabilities, state_reward\n","\n","\n","# Given a particular state S_i, calculate the J*(S_i) Reward\n","def J_star_reward(state, original_state_rewards, prior_state_rewards, gamma, transition_prob):\n","    reward = original_state_rewards[state]\n","    Jsum = 0\n","    for i in range(len(prior_state_rewards)):\n","        Jsum += transition_prob[state][i] * prior_state_rewards[i]\n","    reward += gamma * Jsum\n","    return reward\n","\n","\n","# Given an Epsilon threshold, calculate the Markov System Value Iteration Reward until the difference between consecutive\n","# rewards is smaller than Epsilon.\n","def Markov_System_Value_Iteration(Epsilon, discountFactor, transitionProbabilities, stateRewards):\n","    # At K=1 iteration, the state reward is the reward\n","    discounted_rewards = stateRewards\n","\n","    while True:\n","        # Iterate through every State and calculate the discounted rewards against epsilon\n","        temp = []  # temp holds the value of the current iteration-K rewards.\n","        for i in range(len(discounted_rewards)):\n","            temp.append(J_star_reward(i, stateRewards, discounted_rewards, discountFactor, transitionProbabilities))\n","\n","        # Find the Max difference between K and K-1 iterations of Rewards, and compare it to Epsilon.\n","        # If the difference is smaller than Epsilon, stop calculating. The discounted rewards have converged.\n","        Max_diff = 0\n","        for i in range(len(temp)):\n","            curr_diff = abs(temp[i] - discounted_rewards[i])\n","            Max_diff = curr_diff if curr_diff > Max_diff else Max_diff\n","        # Round the calculated Discounted Rewards to 2 decimal places.\n","        if Max_diff < Epsilon:\n","            temp = [round(elem, 2) for elem in temp]\n","            return temp\n","\n","        discounted_rewards = temp\n","\n","\n","epsilon = 0.001\n","discount_factor = 0.5\n","markovTransitionFileName = '/content/drive/MyDrive/Markov Transition Probabilities - Sheet1.csv'\n","markovRewardFileName = '/content/drive/MyDrive/Markov State Rewards - Sheet1.csv'\n","trans_prob, st_rewards = get_transition_probabilities_and_rewards_from_csv(markovTransitionFileName, markovRewardFileName)\n","print(f\"The Discounted Rewards are: {Markov_System_Value_Iteration(epsilon, discount_factor, trans_prob, st_rewards)}\")\n","\n"]}]}