{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1-KegHs4e7IPtd7SQgF1U2zwVwY0_uH2W","authorship_tag":"ABX9TyMtbheXo50DINwNOn1WjMST"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6ap2R13d_oU","executionInfo":{"status":"ok","timestamp":1710223077012,"user_tz":420,"elapsed":177,"user":{"displayName":"Jerry Chang","userId":"16741169191998840800"}},"outputId":"9063d9e9-1b74-4501-b980-b583c1274818"},"outputs":[{"output_type":"stream","name":"stdout","text":["The Discounted Rewards are: [15.67, 17.1, 19.99]\n","The Optimal Policy is: ['B', 'B', 'A'] for states [S1, S2, S3], respectively.\n"]}],"source":["# This program uses Value Iteration to find the optimal long-term discounted reward for each state and then use that to calculate the optimal policy\n","# in a Markov Decision Process. Value Iteration on MDP is a GREEDY Algorithm (It will take the highest reward at each step).\n","# Note: This program is specific to the Markov Decision Process as described below. For more states or actions, the code must be adjusted to\n","# incorporate Action A, Action B, ..., Action N. For different states,\n","import pandas as pd\n","\n","# transition_probabilities_A = transition probabilities for Action A, transition_probabilities_B = transition probabilities for Action B\n","# transition_probabilities_A = [s1[s1, s2, s3], s2[s1, s2, s3], s3[s1, s2, s3]]\n","# transition_probabilities_B = [s1[s1, s2, s3], s2[s1, s2, s3], s3[s1, s2, s3]]\n","# [s1, s2, s3] original state rewards\n","\n","\n","def get_transition_probabilities_and_rewards_from_csv(transitionFile_A, transitionFile_B, rewardFile):\n","  # Read in CSV files\n","  transition_A_df = pd.read_csv(transitionFile_A, header=None)\n","  transition_B_df = pd.read_csv(transitionFile_B, header=None)\n","  rewards_df = pd.read_csv(rewardFile, header=None)\n","\n","  # Process Markov State Transition Probabilities, ACTION A\n","  transition_probabilities_A = []\n","  for i in range(len(transition_A_df)):\n","    temp = []\n","    ser = pd.Series(transition_A_df.iloc[i])\n","    for j in range(transition_A_df.shape[0]):\n","      temp.append(ser.iloc[j])\n","    transition_probabilities_A.append(temp)\n","\n","  # Process Markov State Transition Probabilities, ACTION B\n","  transition_probabilities_B = []\n","  for i in range(len(transition_B_df)):\n","    temp = []\n","    ser = pd.Series(transition_B_df.iloc[i])\n","    for j in range(transition_B_df.shape[0]):\n","      temp.append(ser.iloc[j])\n","    transition_probabilities_B.append(temp)\n","\n","  # Process Markov State Rewards\n","  state_reward = []\n","  for i in range(len(rewards_df)):\n","    ser = pd.Series(rewards_df.iloc[i])\n","    state_reward.append(ser.iloc[0])\n","\n","  return transition_probabilities_A, transition_probabilities_B, state_reward\n","\n","\n","# Given a particular state S_i, calculate the J*(S_i) Reward\n","def J_star_reward(state, original_state_rewards, prior_state_rewards, gamma, transition_prob_A, transition_prob_B, policyCalculation=False):\n","    reward = original_state_rewards[state]\n","    Jsum_A = 0\n","    Jsum_B = 0\n","    for i in range(len(prior_state_rewards)):\n","        Jsum_A += transition_prob_A[state][i] * prior_state_rewards[i]  # Calculate a Jreward for each action, and take the MAX for most reward gain.\n","        Jsum_B += transition_prob_B[state][i] * prior_state_rewards[i]\n","\n","    reward_A = reward + gamma * Jsum_A\n","    reward_B = reward + gamma * Jsum_B\n","\n","\n","    if not policyCalculation:\n","      return max(reward_A, reward_B)\n","    else:\n","      if reward_A > reward_B:\n","        return 'A'\n","      else:\n","        return 'B'\n","\n","\n","# Given an Epsilon threshold, calculate the Markov System Value Iteration Reward until the difference between consecutive\n","# rewards is smaller than Epsilon.\n","def Markov_Decision_Process_Value_Iteration(Epsilon, discountFactor, transitionProbabilities_A, transition_probabilities_B, stateRewards):\n","    # At K=1 iteration, the state reward is the reward\n","    discounted_rewards = stateRewards\n","\n","    while True:\n","        # Iterate through every State and calculate the discounted rewards against epsilon\n","        temp = []  # temp holds the value of the current iteration-K rewards.\n","        for i in range(len(discounted_rewards)):\n","            temp.append(J_star_reward(i, stateRewards, discounted_rewards, discountFactor, transitionProbabilities_A, transition_probabilities_B))\n","\n","        # Find the Max difference between K and K-1 iterations of Rewards, and compare it to Epsilon.\n","        # If the difference is smaller than Epsilon, stop calculating. The discounted rewards have converged.\n","        Max_diff = 0\n","        for i in range(len(temp)):\n","            curr_diff = abs(temp[i] - discounted_rewards[i])\n","            Max_diff = curr_diff if curr_diff > Max_diff else Max_diff\n","        # Check to see if Error threshold has been reached for max difference between subsequent k-iterations. If so, the rewards have converged.\n","        # Return Long Term Discounted Rewards and the Policy.\n","        # Round the calculated Discounted Rewards to 2 decimal places.\n","        if Max_diff < Epsilon:\n","            longTemDiscountedRewards = [round(elem, 2) for elem in temp]\n","            policy = []\n","            for i in range(len(longTemDiscountedRewards)):\n","              policy.append(J_star_reward(i, stateRewards, longTemDiscountedRewards, discountFactor, transitionProbabilities_A, transition_probabilities_B, True))\n","            return longTemDiscountedRewards, policy\n","\n","        discounted_rewards = temp\n","\n","\n","\n","# Read in the MDP information, consisting of transition probabilities per ACTION (A, B in this exampe MDP)and state rewards\n","epsilon = 0.001\n","discount_factor = 0.9\n","markovTransitionFileName_A = '/content/drive/MyDrive/MDP_A Transition Probabilities - Sheet1.csv'\n","markovTransitionFileName_B = '/content/drive/MyDrive/MDP_B Transition Probabilities - Sheet1.csv'\n","markovRewardFileName = '/content/drive/MyDrive/MDP State Rewards - Sheet1.csv'\n","trans_prob_A, trans_prob_B, st_rewards = get_transition_probabilities_and_rewards_from_csv(markovTransitionFileName_A, markovTransitionFileName_B, markovRewardFileName)\n","\n","# (1) Calculate the Long Term Discounted Rewards of the Markov Decision Problem\n","# (2) Find the Optimal Policy per State (S1, S2, S3)\n","long_term_discounted_rewards, optimal_policy = Markov_Decision_Process_Value_Iteration(epsilon, discount_factor, trans_prob_A, trans_prob_B, st_rewards)\n","print(f\"The Discounted Rewards are: {long_term_discounted_rewards}\")\n","print(f\"The Optimal Policy is: {optimal_policy} for states [S1, S2, S3], respectively.\")\n","\n","\n"]}]}